import os
import time
import json
import logging
from datetime import date
from typing import List, Dict, Any

import requests

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - fb_parser - %(levelname)s - %(message)s",
)
logger = logging.getLogger("fb_parser")

# ---------- –ö–û–ù–§–ò–ì ----------

API_BASE_URL = (os.getenv("API_BASE_URL") or "").rstrip("/")
if not API_BASE_URL:
    raise RuntimeError("API_BASE_URL is not set")

API_SECRET = os.getenv("API_SECRET", "")

# –æ—Ç–∫—É–¥–∞ –±–µ—Ä—ë–º —Å–ø–∏—Å–æ–∫ FB-–≥—Ä—É–ø–ø
FB_GROUPS_API_URL = os.getenv(
    "FB_GROUPS_API_URL",
    f"{API_BASE_URL}/api/fb_groups",
)

APIFY_TOKEN = os.getenv("APIFY_TOKEN")
APIFY_ACTOR_ID = os.getenv("APIFY_ACTOR_ID", "AtBpiepuIUNs2k2ku")

if not APIFY_TOKEN:
    raise RuntimeError("APIFY_TOKEN is not set")

# JSON-–º–∞—Å—Å–∏–≤ cookies –∫–∞–∫ –≤ —Ç–≤–æ—ë–º –ø—Ä–∏–º–µ—Ä–µ –≤—ã—à–µ
FB_COOKIES_JSON = os.getenv("FB_COOKIES_JSON", "[]")
try:
    FB_COOKIES = json.loads(FB_COOKIES_JSON)
except Exception as e:
    logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å FB_COOKIES_JSON: %s", e)
    FB_COOKIES = []

APIFY_MIN_DELAY = int(os.getenv("APIFY_MIN_DELAY", "1"))
APIFY_MAX_DELAY = int(os.getenv("APIFY_MAX_DELAY", "10"))

POLL_INTERVAL_SECONDS = int(os.getenv("POLL_INTERVAL_SECONDS", "900"))  # 15 –º–∏–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

KEYWORDS = [
    "–≤–∞–∫–∞–Ω—Å–∏—è",
    "—Ä–∞–±–æ—Ç–∞",
    "job",
    "hiring",
    "remote",
    "developer",
    "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç",
]

_seen_hashes: set[str] = set()


# ---------- –£–¢–ò–õ–ò–¢–´ ----------

def matches_keywords(text: str | None) -> bool:
    if not text:
        return False
    low = text.lower()
    return any(k in low for k in KEYWORDS)


def today_str() -> str:
    return date.today().isoformat()  # 'YYYY-MM-DD'


def get_fb_groups() -> List[str]:
    """
    –û–∂–∏–¥–∞–µ–º—ã–π –æ—Ç–≤–µ—Ç –º–∏–Ω–∏–∞–ø–ø–∞:
    { "groups": [ { "id": 1, "group_url": "...", "enabled": true }, ... ] }
    """
    try:
        logger.info("–ó–∞–ø—Ä–∞—à–∏–≤–∞—é FB-–≥—Ä—É–ø–ø—ã –∏–∑ %s", FB_GROUPS_API_URL)
        resp = requests.get(FB_GROUPS_API_URL, timeout=30)
        resp.raise_for_status()
        data = resp.json()
    except Exception as e:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å FB-–≥—Ä—É–ø–ø—ã: %s", e)
        return []

    groups = data.get("groups") or []
    urls: List[str] = []
    for g in groups:
        if not g.get("enabled", True):
            continue
        url = (g.get("group_url") or "").strip()
        if url:
            urls.append(url)

    logger.info("üì• –ê–∫—Ç–∏–≤–Ω—ã–µ FB-–≥—Ä—É–ø–ø—ã: %s", urls)
    return urls


def call_apify_for_group(group_url: str) -> List[Dict[str, Any]]:
    """
    –í—ã–∑—ã–≤–∞–µ—Ç actor AtBpiepuIUNs2k2ku –¥–ª—è –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø—ã
    —Å —Ç–µ–º –∂–µ input, —á—Ç–æ —É —Ç–µ–±—è –≤ –∫–æ–Ω—Å–æ–ª–∏:
      - cookie: [ ... ]
      - maxDelay, minDelay
      - proxy.useApifyProxy = true
      - scrapeGroupPosts.groupUrl
      - scrapeUntil = —Å–µ–≥–æ–¥–Ω—è
      - sortType = new_posts
    """
    endpoint = (
        f"https://api.apify.com/v2/acts/{APIFY_ACTOR_ID}/run-sync-get-dataset-items"
        f"?token={APIFY_TOKEN}"
    )

    actor_input = {
        "cookie": FB_COOKIES,
        "maxDelay": APIFY_MAX_DELAY,
        "minDelay": APIFY_MIN_DELAY,
        "proxy": {
            "useApifyProxy": True,
        },
        "scrapeGroupPosts.groupUrl": group_url,
        "scrapeUntil": today_str(),  # –ø–æ—Å—Ç—ã –¥–æ —Å–µ–≥–æ–¥–Ω—è—à–Ω–µ–≥–æ –¥–Ω—è
        "sortType": "new_posts",
    }

    logger.info("‚ñ∂Ô∏è –í—ã–∑—ã–≤–∞—é Apify actor –¥–ª—è –≥—Ä—É–ø–ø—ã: %s", group_url)

    try:
        resp = requests.post(endpoint, json=actor_input, timeout=600)
        resp.raise_for_status()
    except Exception as e:
        logger.error("‚ùå –û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ Apify –¥–ª—è %s: %s", group_url, e)
        return []

    try:
        data = resp.json()
    except Exception as e:
        logger.error("‚ùå JSON-–æ—à–∏–±–∫–∞ –æ—Ç Apify (%s): %s", group_url, e)
        return []

    # run-sync-get-dataset-items –æ–±—ã—á–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª–∏–±–æ —Å–ø–∏—Å–æ–∫, –ª–∏–±–æ –æ–±—ä–µ–∫—Ç —Å items
    if isinstance(data, list):
        items = data
    elif isinstance(data, dict) and "items" in data:
        items = data["items"]
    else:
        logger.warning(
            "–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ Apify –¥–ª—è %s (%s): %r",
            group_url,
            type(data).__name__,
            str(data)[:300],
        )
        items = []

    logger.info("üìÑ Apify –¥–ª—è %s –≤–µ—Ä–Ω—É–ª %d –ø–æ—Å—Ç–æ–≤", group_url, len(items))
    return items


def hash_post(text: str, url: str | None) -> str:
    import hashlib

    h = hashlib.sha256()
    h.update((text or "").encode("utf-8"))
    if url:
        h.update(url.encode("utf-8"))
    return h.hexdigest()


def send_job_to_miniapp(
    text: str,
    post_url: str | None,
    created_at: str | None,
    group_url: str | None,
):
    if not text:
        return

    url = f"{API_BASE_URL}/post"
    headers = {
        "Content-Type": "application/json",
        "X-API-KEY": API_SECRET,
    }

    payload = {
        "source": "facebook",
        "source_name": group_url or "facebook_group",
        "external_id": post_url or (created_at or ""),
        "url": post_url,
        "text": text,
        "created_at": created_at,
    }

    try:
        resp = requests.post(url, json=payload, headers=headers, timeout=30)
        if resp.status_code >= 300:
            logger.error(
                "‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ –º–∏–Ω–∏–∞–ø–ø: %s %s",
                resp.status_code,
                resp.text[:500],
            )
        else:
            logger.info("‚úÖ –í–∞–∫–∞–Ω—Å–∏—è –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –≤ –º–∏–Ω–∏–∞–ø–ø: %s", (post_url or "")[:120])
    except Exception as e:
        logger.error("‚ùå –°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –≤ –º–∏–Ω–∏–∞–ø–ø: %s", e)


# ---------- –û–°–ù–û–í–ù–û–ô –¶–ò–ö–õ ----------

def process_cycle():
    group_urls = get_fb_groups()
    if not group_urls:
        logger.warning("–ù–µ—Ç FB-–≥—Ä—É–ø–ø –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞, –ø—Ä–æ–ø—É—Å–∫–∞—é —Ü–∏–∫–ª")
        return

    total_sent = 0

    for group_url in group_urls:
        items = call_apify_for_group(group_url)

        for item in items:
            # –∏–º–µ–Ω–∞ –ø–æ–ª–µ–π —É –∞–∫—Ç–æ—Ä–∞ –º–æ–≥—É—Ç –±—ã—Ç—å —Å–≤–æ–∏–º–∏ ‚Äî –ø–æ–¥—Å—Ç—Ä–∞—Ö–æ–≤—ã–≤–∞–µ–º—Å—è
            text = (
                item.get("text")
                or item.get("message")
                or item.get("content")
                or item.get("postText")
                or ""
            )
            if not matches_keywords(text):
                continue

            post_url = (
                item.get("postUrl")
                or item.get("url")
                or item.get("post_url")
            )

            created_at = (
                item.get("createdAt")
                or item.get("timestamp")
                or item.get("created_time")
            )

            group_field = (
                item.get("groupUrl")
                or item.get("group_url")
                or group_url
            )

            h = hash_post(text, post_url)
            if h in _seen_hashes:
                logger.info("üîÅ –î—É–±–ª–∏–∫–∞—Ç –ø–æ—Å—Ç–∞ (hash=%s), –ø—Ä–æ–ø—É—Å–∫–∞—é", h)
                continue
            _seen_hashes.add(h)

            send_job_to_miniapp(text, post_url, created_at, group_field)
            total_sent += 1

    logger.info("‚úÖ –¶–∏–∫–ª –∑–∞–≤–µ—Ä—à—ë–Ω, –≤—Å–µ–≥–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: %d", total_sent)


def main():
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ Facebook Job Parser —á–µ—Ä–µ–∑ Apify actor %s", APIFY_ACTOR_ID)
    while True:
        try:
            process_cycle()
        except Exception as e:
            logger.error("‚ùå –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ: %s", e)
        logger.info("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ %d —Å–µ–∫—É–Ω–¥ –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ü–∏–∫–ª–∞", POLL_INTERVAL_SECONDS)
        time.sleep(POLL_INTERVAL_SECONDS)


if __name__ == "__main__":
    main()
