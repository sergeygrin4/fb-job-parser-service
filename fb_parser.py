import os
import time
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse, urljoin

import requests
from bs4 import BeautifulSoup
from requests.exceptions import RequestException, HTTPError

# ----------------- –õ–û–ì–ò -----------------

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - fb_parser - %(levelname)s - %(message)s",
)
log = logging.getLogger("fb_parser")


# ----------------- –ö–û–ù–§–ò–ì -----------------

API_BASE_URL = (os.getenv("API_BASE_URL") or "").rstrip("/")
API_SECRET = os.getenv("API_SECRET", "mvp-secret-key-2024")

KEYWORDS_ENV = os.getenv(
    "KEYWORDS",
    "–≤–∞–∫–∞–Ω—Å–∏—è,—Ä–∞–±–æ—Ç–∞,job,hiring,remote,developer,–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç",
)
KEYWORDS: List[str] = [k.strip().lower() for k in KEYWORDS_ENV.split(",") if k.strip()]

CHECK_INTERVAL_MINUTES = int(os.getenv("CHECK_INTERVAL_MINUTES", "5"))
POSTS_PER_GROUP = int(os.getenv("POSTS_PER_GROUP", "20"))

FB_COOKIES_JSON = os.getenv("FB_COOKIES_JSON", "")

FB_USER_AGENT = os.getenv(
    "FB_USER_AGENT",
    "Mozilla/5.0 (Linux; Android 10; SM-G973F) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/120.0.0.0 Mobile Safari/537.36",
)

FB_BASIC_HOST = os.getenv("FB_BASIC_HOST", "mbasic.facebook.com")


# ----------------- –ö–£–ö–ò -----------------


def load_cookies() -> Optional[Dict[str, str]]:
    if not FB_COOKIES_JSON:
        log.warning("‚ö†Ô∏è FB_COOKIES_JSON –Ω–µ –∑–∞–¥–∞–Ω ‚Äî Facebook, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –ø–æ–∫–∞–∂–µ—Ç –ª–æ–≥–∏–Ω/–∫–∞–ø—á—É")
        return None

    try:
        raw = json.loads(FB_COOKIES_JSON)
    except json.JSONDecodeError as e:
        log.error(f"‚ùå –ù–µ –º–æ–≥—É —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å FB_COOKIES_JSON –∫–∞–∫ JSON: {e}")
        return None

    cookies: Dict[str, str]
    if isinstance(raw, dict):
        cookies = {k: str(v) for k, v in raw.items()}
    elif isinstance(raw, list):
        cookies = {
            c["name"]: str(c["value"])
            for c in raw
            if isinstance(c, dict) and "name" in c and "value" in c
        }
    else:
        log.error("‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç FB_COOKIES_JSON (–æ–∂–∏–¥–∞–ª—Å—è dict –∏–ª–∏ list)")
        return None

    if not cookies:
        log.warning("‚ö†Ô∏è –í FB_COOKIES_JSON –Ω–µ—Ç –Ω–∏ –æ–¥–Ω–æ–π cookies-–ø–∞—Ä—ã")
        return None

    log.info(f"Cookies –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ö–ª—é—á–∏: {list(cookies.keys())}")
    return cookies


def create_fb_session(cookies: Optional[Dict[str, str]]) -> requests.Session:
    s = requests.Session()
    s.headers.update(
        {
            "User-Agent": FB_USER_AGENT,
            "Accept-Language": "en-US,en;q=0.9,ru;q=0.8",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        }
    )
    if cookies:
        s.cookies.update(cookies)
    return s


# ----------------- –ì–†–£–ü–ü–´ –ò–ó –ú–ò–ù–ò–ê–ü–ü–ê -----------------


def get_fb_groups() -> List[Dict]:
    if not API_BASE_URL:
        log.error("‚ùå API_BASE_URL –Ω–µ –∑–∞–¥–∞–Ω ‚Äî –Ω–µ –º–æ–≥—É –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≥—Ä—É–ø–ø.")
        return []

    url = f"{API_BASE_URL}/api/groups"
    log.info(f"API –≥—Ä—É–ø–ø: {url}")

    try:
        resp = requests.get(url, timeout=30)
        resp.raise_for_status()
    except RequestException as e:
        log.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≥—Ä—É–ø–ø—ã: {e}")
        return []

    try:
        data = resp.json()
    except ValueError:
        log.error("‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –≥—Ä—É–ø–ø")
        return []

    all_groups = data.get("groups") or []
    enabled_groups = [g for g in all_groups if g.get("enabled")]

    fb_groups: List[Dict] = []
    skipped_non_fb: List[str] = []

    for g in enabled_groups:
        gid = (g.get("group_id") or "").strip()
        low = gid.lower()

        if "t.me/" in low or "telegram.me" in low:
            skipped_non_fb.append(gid)
            continue

        if "facebook.com" in low or "fb.com" in low:
            fb_groups.append(g)
        else:
            skipped_non_fb.append(gid)

    log.info(
        f"–í—Å–µ–≥–æ –≥—Ä—É–ø–ø –∏–∑ API: {len(all_groups)}; –∞–∫—Ç–∏–≤–Ω—ã—Ö: {len(enabled_groups)}; facebook-–≥—Ä—É–ø–ø: {len(fb_groups)}"
    )
    if skipped_non_fb:
        log.info(f"–ü—Ä–æ–ø—É—â–µ–Ω—ã –Ω–µ-facebook –∏—Å—Ç–æ—á–Ω–∏–∫–∏: {skipped_non_fb}")

    return fb_groups


# ----------------- –£–¢–ò–õ–ò–¢–´ -----------------


def normalize_group_link_to_basic(group_link: str) -> str:
    group_link = (group_link or "").strip()
    if not group_link:
        return f"https://{FB_BASIC_HOST}/groups"

    if not group_link.startswith("http://") and not group_link.startswith("https://"):
        return f"https://{FB_BASIC_HOST}/groups/{group_link}"

    parsed = urlparse(group_link)
    path = parsed.path or "/"
    return f"https://{FB_BASIC_HOST}{path}"


def matches_keywords(text: str) -> bool:
    if not text:
        return False
    low = text.lower()
    return any(k in low for k in KEYWORDS)


# ----------------- –ü–ê–†–°–ò–ù–ì mbasic.facebook.com -----------------


def fetch_group_html(session: requests.Session, basic_url: str) -> Optional[str]:
    try:
        log.info(f"üîé –ó–∞–≥—Ä—É–∂–∞—é basic-–≥—Ä—É–ø–ø—É: {basic_url}")
        resp = session.get(basic_url, timeout=30, allow_redirects=False)
    except RequestException as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {basic_url}: {e}")
        return None

    if 300 <= resp.status_code < 400:
        loc = resp.headers.get("Location", "")
        log.warning(
            f"‚ö†Ô∏è –†–µ–¥–∏—Ä–µ–∫—Ç {resp.status_code} —Å {resp.url} –Ω–∞ {loc} ‚Äî "
            f"Facebook –Ω–µ —Ö–æ—á–µ—Ç –æ—Ç–¥–∞–≤–∞—Ç—å basic-—Å—Ç—Ä–∞–Ω–∏—Ü—É"
        )
        return None

    try:
        resp.raise_for_status()
    except HTTPError as e:
        log.error(f"‚ùå HTTP –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {basic_url}: {e} (url={resp.url})")
        # –ø–æ–ø—Ä–æ–±—É–µ–º –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –∫—É—Å–æ–∫ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –¥–µ–±–∞–≥–∞
        snippet = resp.text[:500].replace("\n", " ")
        log.warning(f"üîç –§—Ä–∞–≥–º–µ–Ω—Ç HTML –ø—Ä–∏ –æ—à–∏–±–∫–µ: {snippet}")
        return None

    # debug: –ø–µ—Ä–≤—ã–π –∫—É—Å–æ–∫ HTML, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, —á—Ç–æ –∑–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–∏—Ö–æ–¥–∏—Ç
    snippet = resp.text[:500].replace("\n", " ")
    log.info(f"üîç –ü–µ—Ä–≤—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç HTML ({len(resp.text)} —Å–∏–º–≤–æ–ª–æ–≤): {snippet}")

    return resp.text


def extract_posts_from_basic_html(
    html: str, base_url: str
) -> List[Tuple[str, Optional[str], Optional[datetime]]]:
    soup = BeautifulSoup(html, "lxml")
    posts: List[Tuple[str, Optional[str], Optional[datetime]]] = []

    # 1. –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –ø–æ—Å—Ç–æ–≤ –≥—Ä—É–ø–ø—ã
    stories_container = soup.find(id="m_group_stories_container")
    if stories_container:
        candidates = stories_container.find_all("div", recursive=False)
    else:
        candidates = []

    # 2. –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî fallback: —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
    if not candidates:
        candidates = soup.find_all("article")

    if not candidates:
        candidates = soup.find_all("div", attrs={"data-ft": True, "role": "article"})

    if not candidates:
        candidates = soup.find_all("div", attrs={"data-ft": True})

    if not candidates:
        candidates = [
            d
            for d in soup.find_all("div")
            if (d.get("id") or "").startswith("m_story")
        ]

    if not candidates:
        log.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –ø–æ—Å—Ç–æ–≤ –≤ basic HTML")
        return posts

    for block in candidates[:POSTS_PER_GROUP]:
        # –∏–Ω–æ–≥–¥–∞ –≤–Ω—É—Ç—Ä–∏ –µ—â—ë –æ–¥–∏–Ω div —Å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ —Ç–µ–∫—Å—Ç–æ–º
        content_block = block
        inner = block.find("div")
        if inner and inner.get_text(strip=True):
            content_block = inner

        text = content_block.get_text(" ", strip=True)
        if not text:
            continue

        post_url: Optional[str] = None
        for a in block.find_all("a", href=True):
            href = a["href"]
            if (
                "story.php" in href
                or "/permalink/" in href
                or "/posts/" in href
                or "/groups/" in href and "view=permalink" in href
            ):
                post_url = urljoin(base_url, href.split("&", 1)[0])
                break

        created_at: Optional[datetime] = None
        abbr = block.find("abbr")
        if abbr and abbr.has_attr("data-utime"):
            try:
                ts = int(abbr["data-utime"])
                created_at = datetime.utcfromtimestamp(ts)
            except Exception:
                created_at = None
        else:
            span = block.find("span", attrs={"data-utime": True})
            if span:
                try:
                    ts = int(span["data-utime"])
                    created_at = datetime.utcfromtimestamp(ts)
                except Exception:
                    created_at = None

        posts.append((text, post_url, created_at))

    log.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(posts)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ –≤ basic HTML")
    return posts


# ----------------- –û–¢–ü–†–ê–í–ö–ê –í–ê–ö–ê–ù–°–ò–ô -----------------


def send_job_to_api(
    source_name: str,
    external_id: str,
    url: Optional[str],
    text: str,
    created_at: Optional[datetime],
) -> None:
    if not API_BASE_URL:
        log.error("‚ùå API_BASE_URL –Ω–µ –∑–∞–¥–∞–Ω ‚Äî –Ω–µ –º–æ–≥—É –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –≤–∞–∫–∞–Ω—Å–∏—é")
        return

    endpoint = f"{API_BASE_URL}/post"
    headers = {"Content-Type": "application/json"}
    if API_SECRET:
        headers["X-API-KEY"] = API_SECRET

    payload = {
        "source": "facebook",
        "source_name": source_name,
        "external_id": str(external_id),
        "url": url,
        "text": text,
        "created_at": created_at.isoformat() if created_at else None,
    }

    try:
        resp = requests.post(endpoint, headers=headers, json=payload, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        status = data.get("status")
        if status == "duplicate":
            log.info(f"üîÅ –î—É–±–ª–∏–∫–∞—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ {external_id} ({source_name})")
        else:
            log.info(f"‚úÖ –í–∞–∫–∞–Ω—Å–∏—è –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –≤ API ({source_name} / {external_id})")
    except RequestException as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ API: {e}")


# ----------------- –ü–ê–†–°–ò–ù–ì –û–î–ù–û–ô –ì–†–£–ü–ü–´ -----------------


def parse_one_group(
    session: requests.Session,
    group_link: str,
    group_name: str,
) -> int:
    basic_url = normalize_group_link_to_basic(group_link)
    log.info(f"üîç –ü–∞—Ä—Å–∏–º –≥—Ä—É–ø–ø—É: {group_name} ({group_link}) ‚Üí {basic_url}")

    html = fetch_group_html(session, basic_url)
    if not html:
        return 0

    posts = extract_posts_from_basic_html(html, base_url=f"https://{FB_BASIC_HOST}")
    sent = 0

    for text, post_url, created_at in posts:
        if not matches_keywords(text):
            continue

        base = group_link.split("?", 1)[0]
        ext = f"{base}|{post_url or text[:50]}"
        external_id = str(abs(hash(ext)))

        send_job_to_api(
            source_name=group_name or group_link,
            external_id=external_id,
            url=post_url,
            text=text,
            created_at=created_at,
        )
        sent += 1

    log.info(f"üì¶ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {sent} –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –≥—Ä—É–ø–ø—ã {group_name}")
    return sent


# ----------------- –ì–õ–ê–í–ù–´–ô –¶–ò–ö–õ -----------------


def run_once():
    if not API_BASE_URL:
        log.error("‚ùå API_BASE_URL –Ω–µ –∑–∞–¥–∞–Ω ‚Äî –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é —Ü–∏–∫–ª")
        return

    log.info(f"API: {API_BASE_URL}")
    log.info(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {KEYWORDS}")

    cookies = load_cookies()
    session = create_fb_session(cookies)

    groups = get_fb_groups()
    total_sent = 0

    for g in groups:
        group_link = g.get("group_id") or ""
        group_name = g.get("group_name") or group_link
        try:
            total_sent += parse_one_group(session, group_link, group_name)
            time.sleep(2)
        except Exception as e:
            log.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {group_link}: {e}")

    log.info(f"‚úÖ –¶–∏–∫–ª –∑–∞–≤–µ—Ä—à—ë–Ω. –í—Å–µ–≥–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {total_sent}")


def main():
    log.info("üöÄ –ó–∞–ø—É—Å–∫ Facebook Job Parser (mbasic.facebook.com, —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä)")
    while True:
        try:
            run_once()
        except Exception as e:
            log.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")
        log.info(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {CHECK_INTERVAL_MINUTES} –º–∏–Ω—É—Ç...")
        time.sleep(CHECK_INTERVAL_MINUTES * 60)


if __name__ == "__main__":
    main()
