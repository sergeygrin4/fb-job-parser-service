# fb_parser.py
import os
import time
import logging
import hashlib
from urllib.parse import urlparse

from facebook_scraper import get_posts
import requests

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - fb_parser - %(levelname)s - %(message)s",
)
log = logging.getLogger(__name__)

API_BASE_URL = os.getenv("API_BASE_URL")  # —Ç–∏–ø–∞ "https://telegram-job-parser-production.up.railway.app"
API_SECRET = os.getenv("API_SECRET", "mvp-secret-key-2024-xyz")

JOB_KEYWORDS = [
    kw.strip().lower()
    for kw in os.getenv("JOB_KEYWORDS", "–≤–∞–∫–∞–Ω—Å–∏—è,—Ä–∞–±–æ—Ç–∞,job,hiring,remote,developer").split(",")
    if kw.strip()
]

CHECK_INTERVAL_MINUTES = int(os.getenv("CHECK_INTERVAL_MINUTES", "5"))
MAX_POSTS_PER_GROUP = int(os.getenv("MAX_POSTS_PER_GROUP", "20"))

FACEBOOK_COOKIES = os.getenv("FACEBOOK_COOKIES")  # —Å—ã—Ä—ã–µ cookies —Å—Ç—Ä–æ–∫–æ–π, –µ—Å–ª–∏ –Ω–∞–¥–æ


def get_cookies_dict():
    """
    –ü—Ä–æ—Å—Ç–æ–π —Ä–∞–∑–±–æ—Ä cookies —Ñ–æ—Ä–º–∞—Ç–∞ "key1=value1; key2=value2"
    """
    if not FACEBOOK_COOKIES:
        return None
    cookies = {}
    for part in FACEBOOK_COOKIES.split(";"):
        part = part.strip()
        if not part or "=" not in part:
            continue
        k, v = part.split("=", 1)
        cookies[k.strip()] = v.strip()
    return cookies


def get_fb_groups():
    """
    –¢—è–Ω–µ–º —Å–ø–∏—Å–æ–∫ FB-–≥—Ä—É–ø–ø –∏–∑ miniapp-—Å–µ—Ä–≤–∏—Å–∞.
    GET /api/groups
    """
    url = f"{API_BASE_URL}/api/groups"
    resp = requests.get(url)
    resp.raise_for_status()
    data = resp.json()
    groups = []
    for g in data.get("groups", []):
        if g.get("enabled"):
            groups.append((g["group_id"], g["group_name"]))
    return groups


def text_matches_keywords(text: str) -> bool:
    t = (text or "").lower()
    return any(kw in t for kw in JOB_KEYWORDS)


def build_external_id(group_id: str, post_id: str) -> str:
    # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    raw = f"fb:{group_id}:{post_id}"
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()


def send_job(group_name: str, group_link: str, post) -> None:
    """
    –û—Ç–ø—Ä–∞–≤–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ miniapp /post
    """
    post_id = post.get("post_id") or post.get("post_url")
    text = post.get("text") or ""
    post_url = post.get("post_url") or group_link

    external_id = build_external_id(group_link, str(post_id))

    payload = {
        "source": "facebook",
        "source_name": group_name,
        "external_id": external_id,
        "url": post_url,
        "text": text,
        "created_at": post.get("time").isoformat() if post.get("time") else None,
    }

    headers = {
        "Content-Type": "application/json",
        "X-API-KEY": API_SECRET,
    }

    resp = requests.post(f"{API_BASE_URL}/post", json=payload, headers=headers)
    if resp.status_code == 200:
        data = resp.json()
        if data.get("status") == "duplicate":
            log.info(f"üîÅ –£–∂–µ –µ—Å—Ç—å —Ç–∞–∫–æ–π –ø–æ—Å—Ç: {external_id}")
        else:
            log.info(f"‚úÖ –ù–æ–≤–∞—è –≤–∞–∫–∞–Ω—Å–∏—è –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞: {external_id}")
    else:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤–∞–∫–∞–Ω—Å–∏–∏: {resp.status_code} {resp.text}")


def parse_group(group_link: str, group_name: str, cookies: dict | None):
    """
    –ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø—ã
    """
    log.info(f"üîç –ü–∞—Ä—Å–∏–º –≥—Ä—É–ø–ø—É: {group_name} ({group_link})")

    # facebook-scraper –º–æ–∂–µ—Ç –ø—Ä–∏–Ω–∏–º–∞—Ç—å –ª–∏–±–æ group=ID, –ª–∏–±–æ account=...
    parsed = urlparse(group_link)
    group = parsed.path.strip("/")

    count = 0
    for post in get_posts(
        group=group,
        pages=1,
        cookies=cookies,
        options={"allow_extra_requests": False},
    ):
        text = post.get("text") or ""
        if not text_matches_keywords(text):
            continue

        send_job(group_name, group_link, post)
        count += 1

        if count >= MAX_POSTS_PER_GROUP:
            break

    log.info(f"üìå –î–ª—è {group_name} –Ω–∞–π–¥–µ–Ω–æ {count} –ø–æ—Å—Ç–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º")
    return count


def run_loop():
    cookies = get_cookies_dict()
    log.info("üöÄ –ó–∞–ø—É—Å–∫ Facebook Job Parser")

    while True:
        try:
            groups = get_fb_groups()
            log.info(f"–ù–∞–π–¥–µ–Ω–æ {len(groups)} –∞–∫—Ç–∏–≤–Ω—ã—Ö –≥—Ä—É–ø–ø")

            total_posts = 0
            for group_link, group_name in groups:
                total_posts += parse_group(group_link, group_name, cookies)
                time.sleep(2)

            log.info(f"‚úÖ –¶–∏–∫–ª –∑–∞–≤–µ—Ä—à—ë–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {total_posts} –ø–æ—Å—Ç–æ–≤")
        except Exception as e:
            log.exception(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")

        log.info(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {CHECK_INTERVAL_MINUTES} –º–∏–Ω—É—Ç...")
        time.sleep(CHECK_INTERVAL_MINUTES * 60)


if __name__ == "__main__":
    run_loop()
