import os
import time
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse, urljoin

import requests
from bs4 import BeautifulSoup
from requests.exceptions import RequestException, HTTPError

# ----------------- –õ–û–ì–ò -----------------

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - fb_parser - %(levelname)s - %(message)s",
)
log = logging.getLogger("fb_parser")


# ----------------- –ö–û–ù–§–ò–ì -----------------

API_BASE_URL = (os.getenv("API_BASE_URL") or "").rstrip("/")
API_SECRET = os.getenv("API_SECRET", "mvp-secret-key-2024")

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)
KEYWORDS_ENV = os.getenv(
    "KEYWORDS",
    "–≤–∞–∫–∞–Ω—Å–∏—è,—Ä–∞–±–æ—Ç–∞,job,hiring,remote,developer,–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç",
)
KEYWORDS: List[str] = [k.strip().lower() for k in KEYWORDS_ENV.split(",") if k.strip()]

CHECK_INTERVAL_MINUTES = int(os.getenv("CHECK_INTERVAL_MINUTES", "5"))
POSTS_PER_GROUP = int(os.getenv("POSTS_PER_GROUP", "20"))

FB_COOKIES_JSON = os.getenv("FB_COOKIES_JSON", "")
FB_USER_AGENT = os.getenv(
    "FB_USER_AGENT",
    # –ú–æ–±–∏–ª—å–Ω—ã–π Chrome –ø–æ–¥ Android, —á—Ç–æ–±—ã FB –Ω–µ —Ä–µ–¥–∏—Ä–µ–∫—Ç–∏–ª –Ω–∞ –¥–µ—Å–∫—Ç–æ–ø
    "Mozilla/5.0 (Linux; Android 10; SM-G973F) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/120.0 Mobile Safari/537.36",
)



# ----------------- –ö–£–ö–ò -----------------


def load_cookies() -> Optional[Dict[str, str]]:
    """
    –ß–∏—Ç–∞–µ—Ç FB_COOKIES_JSON. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–±–∞ —Ñ–æ—Ä–º–∞—Ç–∞:
      1) {"c_user": "...", "xs": "...", ...}
      2) [{"name": "c_user", "value": "...", ...}, ...]
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict name -> value.
    """
    if not FB_COOKIES_JSON:
        log.warning("‚ö†Ô∏è FB_COOKIES_JSON –Ω–µ –∑–∞–¥–∞–Ω ‚Äî Facebook, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –ø–æ–∫–∞–∂–µ—Ç –ª–æ–≥–∏–Ω/–∫–∞–ø—á—É")
        return None

    try:
        raw = json.loads(FB_COOKIES_JSON)
    except json.JSONDecodeError as e:
        log.error(f"‚ùå –ù–µ –º–æ–≥—É —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å FB_COOKIES_JSON –∫–∞–∫ JSON: {e}")
        return None

    cookies: Dict[str, str]
    if isinstance(raw, dict):
        cookies = {k: str(v) for k, v in raw.items()}
    elif isinstance(raw, list):
        cookies = {
            c["name"]: str(c["value"])
            for c in raw
            if isinstance(c, dict) and "name" in c and "value" in c
        }
    else:
        log.error("‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç FB_COOKIES_JSON (–æ–∂–∏–¥–∞–ª—Å—è dict –∏–ª–∏ list)")
        return None

    if not cookies:
        log.warning("‚ö†Ô∏è –í FB_COOKIES_JSON –Ω–µ—Ç –Ω–∏ –æ–¥–Ω–æ–π cookies-–ø–∞—Ä—ã")
        return None

    log.info(f"Cookies –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ö–ª—é—á–∏: {list(cookies.keys())}")
    return cookies


def create_fb_session(cookies: Optional[Dict[str, str]]) -> requests.Session:
    s = requests.Session()
    s.headers.update(
        {
            "User-Agent": FB_USER_AGENT,
            "Accept-Language": "en-US,en;q=0.9,ru;q=0.8",
        }
    )
    if cookies:
        s.cookies.update(cookies)
    return s


# ----------------- –ì–†–£–ü–ü–´ –ò–ó –ú–ò–ù–ò–ê–ü–ü–ê -----------------


def get_fb_groups() -> List[Dict]:
    """
    –ó–∞–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ –≥—Ä—É–ø–ø –∏–∑ –º–∏–Ω–∏–∞–ø–ø–∞: GET {API_BASE_URL}/api/groups.

    –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
      {
        "groups": [
          {
            "id": ...,
            "group_id": "https://www.facebook.com/groups/ProjectAmazon",
            "group_name": "...",
            "enabled": true
          },
          ...
        ]
      }

    –ó–¥–µ—Å—å –º—ã –æ—Å—Ç–∞–≤–ª—è–µ–º –¢–û–õ–¨–ö–û –∞–∫—Ç–∏–≤–Ω—ã–µ facebook-–≥—Ä—É–ø–ø—ã:
      - enabled = true
      - group_id —Å–æ–¥–µ—Ä–∂–∏—Ç facebook.com –∏–ª–∏ fb.com
      - –∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç t.me / telegram.me
    """
    if not API_BASE_URL:
        log.error("‚ùå API_BASE_URL –Ω–µ –∑–∞–¥–∞–Ω ‚Äî –Ω–µ –º–æ–≥—É –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≥—Ä—É–ø–ø.")
        return []

    url = f"{API_BASE_URL}/api/groups"
    log.info(f"API –≥—Ä—É–ø–ø: {url}")

    try:
        resp = requests.get(url, timeout=30)
        resp.raise_for_status()
    except RequestException as e:
        log.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≥—Ä—É–ø–ø—ã: {e}")
        return []

    try:
        data = resp.json()
    except ValueError:
        log.error("‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –≥—Ä—É–ø–ø")
        return []

    all_groups = data.get("groups") or []
    enabled_groups = [g for g in all_groups if g.get("enabled")]

    fb_groups: List[Dict] = []
    skipped_non_fb: List[str] = []

    for g in enabled_groups:
        gid = (g.get("group_id") or "").strip()
        low = gid.lower()

        # –¢–µ–ª–µ–≥—Ä–∞–º ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–∏—Ö –∑–∞–±–µ—Ä—ë—Ç tg_parser)
        if "t.me/" in low or "telegram.me" in low:
            skipped_non_fb.append(gid)
            continue

        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ URL/–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã, –≥–¥–µ —è–≤–Ω–æ –≤–∏–¥–Ω–æ facebook / fb
        if "facebook.com" in low or "fb.com" in low:
            fb_groups.append(g)
        else:
            skipped_non_fb.append(gid)

    log.info(
        f"–í—Å–µ–≥–æ –≥—Ä—É–ø–ø –∏–∑ API: {len(all_groups)}; –∞–∫—Ç–∏–≤–Ω—ã—Ö: {len(enabled_groups)}; facebook-–≥—Ä—É–ø–ø: {len(fb_groups)}"
    )
    if skipped_non_fb:
        log.info(f"–ü—Ä–æ–ø—É—â–µ–Ω—ã –Ω–µ-facebook –∏—Å—Ç–æ—á–Ω–∏–∫–∏: {skipped_non_fb}")

    return fb_groups


# ----------------- –£–¢–ò–õ–ò–¢–´ -----------------


def normalize_group_link_to_mobile(group_link: str) -> str:
    """
    –î–µ–ª–∞–µ—Ç –∏–∑ https://www.facebook.com/groups/ProjectAmazon
    ‚Üí https://m.facebook.com/groups/ProjectAmazon
    """
    group_link = group_link.strip()
    if not group_link.startswith("http"):
        # –µ—Å–ª–∏ –≤–¥—Ä—É–≥ –∫—Ç–æ-—Ç–æ –∑–∞—Å—É–Ω—É–ª –ø—Ä–æ—Å—Ç–æ ID/–∏–º—è, —Å–æ–±–µ—Ä—ë–º —Å–∞–º–∏
        return f"https://m.facebook.com/groups/{group_link}"

    parsed = urlparse(group_link)
    host = parsed.netloc
    path = parsed.path or "/"

    # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ö–æ—Å—Ç –Ω–∞ m.facebook.com
    if "facebook.com" in host and not host.startswith("m.facebook.com"):
        host = "m.facebook.com"

    mobile_url = f"https://{host}{path}"
    return mobile_url


def matches_keywords(text: str) -> bool:
    if not text:
        return False
    low = text.lower()
    return any(k in low for k in KEYWORDS)


# ----------------- –ü–ê–†–°–ò–ù–ì m.facebook.com -----------------


def fetch_group_html(session: requests.Session, mobile_url: str) -> Optional[str]:
    try:
        log.info(f"üîé –ó–∞–≥—Ä—É–∂–∞—é –º–æ–±–∏–ª—å–Ω—É—é –≥—Ä—É–ø–ø—É: {mobile_url}")
        resp = session.get(mobile_url, timeout=30)
        resp.raise_for_status()
    except HTTPError as e:
        log.error(f"‚ùå HTTP –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {mobile_url}: {e}")
        return None
    except RequestException as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {mobile_url}: {e}")
        return None

    return resp.text


def extract_posts_from_mobile_html(html: str, base_url: str) -> List[Tuple[str, Optional[str], Optional[datetime]]]:
    """
    –û—á–µ–Ω—å –≥—Ä—É–±—ã–π –º–æ–±–∏–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä:

    - –∏—â–µ–º –±–ª–æ–∫–∏-–ø–æ—Å—Ç—ã –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —ç–≤—Ä–∏—Å—Ç–∏–∫–∞–º:
        * article
        * div[data-ft][role=article]
        * div —Å id, –ø–æ—Ö–æ–∂–∏–º –Ω–∞ "m_story"
    - —Å–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞
    - –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ permalink
    - –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º –≤—Ä–µ–º—è –∏–∑ abbr[data-utime] / span[data-utime]
    """
    soup = BeautifulSoup(html, "lxml")
    posts: List[Tuple[str, Optional[str], Optional[datetime]]] = []

    # 1) article
    containers = soup.find_all("article")

    # 2) div[data-ft][role=article]
    if not containers:
        containers = soup.find_all("div", attrs={"data-ft": True, "role": "article"})

    # 3) –ª—é–±—ã–µ div —Å data-ft
    if not containers:
        containers = soup.find_all("div", attrs={"data-ft": True})

    # 4) –ø–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–Ω—Å ‚Äî div, id –∫–æ—Ç–æ—Ä—ã—Ö –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ story
    if not containers:
        containers = [
            d
            for d in soup.find_all("div")
            if (d.get("id") or "").startswith("m_story")
        ]

    if not containers:
        log.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –ø–æ—Å—Ç–æ–≤ –≤ –º–æ–±–∏–ª—å–Ω–æ–º HTML")
        return posts

    for block in containers[:POSTS_PER_GROUP]:
        text = block.get_text(" ", strip=True)
        if not text:
            continue

        # permalink
        post_url: Optional[str] = None
        for a in block.find_all("a", href=True):
            href = a["href"]
            if "story.php" in href or "/permalink/" in href or "/posts/" in href:
                post_url = urljoin(base_url, href.split("?", 1)[0])
                break

        # –≤—Ä–µ–º—è
        created_at: Optional[datetime] = None
        abbr = block.find("abbr")
        if abbr and abbr.has_attr("data-utime"):
            try:
                ts = int(abbr["data-utime"])
                created_at = datetime.utcfromtimestamp(ts)
            except Exception:
                created_at = None
        else:
            span = block.find("span", attrs={"data-utime": True})
            if span:
                try:
                    ts = int(span["data-utime"])
                    created_at = datetime.utcfromtimestamp(ts)
                except Exception:
                    created_at = None

        posts.append((text, post_url, created_at))

    log.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(posts)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ –≤ –º–æ–±–∏–ª—å–Ω–æ–º HTML")
    return posts


# ----------------- –û–¢–ü–†–ê–í–ö–ê –í–ê–ö–ê–ù–°–ò–ô –í –ú–ò–ù–ò–ê–ü–ü -----------------


def send_job_to_api(
    source_name: str,
    external_id: str,
    url: Optional[str],
    text: str,
    created_at: Optional[datetime],
) -> None:
    if not API_BASE_URL:
        log.error("‚ùå API_BASE_URL –Ω–µ –∑–∞–¥–∞–Ω ‚Äî –Ω–µ –º–æ–≥—É –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –≤–∞–∫–∞–Ω—Å–∏—é")
        return

    endpoint = f"{API_BASE_URL}/post"
    headers = {
        "Content-Type": "application/json",
    }
    if API_SECRET:
        headers["X-API-KEY"] = API_SECRET

    payload = {
        "source": "facebook",
        "source_name": source_name,
        "external_id": str(external_id),
        "url": url,
        "text": text,
        "created_at": created_at.isoformat() if created_at else None,
    }

    try:
        resp = requests.post(endpoint, headers=headers, json=payload, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        status = data.get("status")
        if status == "duplicate":
            log.info(f"üîÅ –î—É–±–ª–∏–∫–∞—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ {external_id} ({source_name})")
        else:
            log.info(f"‚úÖ –í–∞–∫–∞–Ω—Å–∏—è –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –≤ API ({source_name} / {external_id})")
    except RequestException as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ API: {e}")


# ----------------- –ü–ê–†–°–ò–ù–ì –û–î–ù–û–ô –ì–†–£–ü–ü–´ -----------------


def parse_one_group(
    session: requests.Session,
    group_link: str,
    group_name: str,
) -> int:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π.
    """
    mobile_url = normalize_group_link_to_mobile(group_link)
    log.info(f"üîç –ü–∞—Ä—Å–∏–º –≥—Ä—É–ø–ø—É: {group_name} ({group_link}) ‚Üí {mobile_url}")

    html = fetch_group_html(session, mobile_url)
    if not html:
        return 0

    posts = extract_posts_from_mobile_html(html, base_url="https://m.facebook.com")
    sent = 0

    for text, post_url, created_at in posts:
        if not matches_keywords(text):
            continue

        # —Ñ–æ—Ä–º–∏—Ä—É–µ–º external_id –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ + —Å—Å—ã–ª–∫–∏ –∏–ª–∏ –∫—É—Å–∫–∞ —Ç–µ–∫—Å—Ç–∞
        base = group_link.split("?", 1)[0]
        ext = f"{base}|{post_url or text[:50]}"
        external_id = str(abs(hash(ext)))

        send_job_to_api(
            source_name=group_name or group_link,
            external_id=external_id,
            url=post_url,
            text=text,
            created_at=created_at,
        )
        sent += 1

    log.info(f"üì¶ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {sent} –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –≥—Ä—É–ø–ø—ã {group_name}")
    return sent


# ----------------- –ì–õ–ê–í–ù–´–ô –¶–ò–ö–õ -----------------


def run_once():
    if not API_BASE_URL:
        log.error("‚ùå API_BASE_URL –Ω–µ –∑–∞–¥–∞–Ω ‚Äî –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é —Ü–∏–∫–ª")
        return

    log.info(f"API: {API_BASE_URL}")
    log.info(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {KEYWORDS}")

    cookies = load_cookies()
    session = create_fb_session(cookies)

    groups = get_fb_groups()
    total_sent = 0

    for g in groups:
        group_link = g.get("group_id") or ""
        group_name = g.get("group_name") or group_link
        try:
            total_sent += parse_one_group(session, group_link, group_name)
            time.sleep(2)
        except Exception as e:
            log.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {group_link}: {e}")

    log.info(f"‚úÖ –¶–∏–∫–ª –∑–∞–≤–µ—Ä—à—ë–Ω. –í—Å–µ–≥–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {total_sent}")


def main():
    log.info("üöÄ –ó–∞–ø—É—Å–∫ Facebook Job Parser (–º–æ–±–∏–ª—å–Ω—ã–π m.facebook.com)")

    while True:
        try:
            run_once()
        except Exception as e:
            log.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")
        log.info(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {CHECK_INTERVAL_MINUTES} –º–∏–Ω—É—Ç...")
        time.sleep(CHECK_INTERVAL_MINUTES * 60)


if __name__ == "__main__":
    main()
