import os
import time
import json
import logging
from datetime import date, datetime
from typing import List, Dict, Any

import requests

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - fb_parser - %(levelname)s - %(message)s",
)
logger = logging.getLogger("fb_parser")

# ---------- –ö–û–ù–§–ò–ì ----------

API_BASE_URL = (os.getenv("API_BASE_URL") or "").rstrip("/")
if not API_BASE_URL:
    raise RuntimeError("API_BASE_URL is not set")

API_SECRET = os.getenv("API_SECRET", "")

# –æ—Ç–∫—É–¥–∞ –±–µ—Ä—ë–º —Å–ø–∏—Å–æ–∫ FB-–≥—Ä—É–ø–ø
FB_GROUPS_API_URL = os.getenv(
    "FB_GROUPS_API_URL",
    f"{API_BASE_URL}/api/fb_groups",
)

APIFY_TOKEN = os.getenv("APIFY_TOKEN")
APIFY_ACTOR_ID = os.getenv("APIFY_ACTOR_ID", "AtBpiepuIUNs2k2ku")

if not APIFY_TOKEN:
    raise RuntimeError("APIFY_TOKEN is not set")

# JSON-–º–∞—Å—Å–∏–≤ cookies –∫–∞–∫ –≤ —Ç–≤–æ—ë–º –ø—Ä–∏–º–µ—Ä–µ –≤—ã—à–µ
FB_COOKIES_JSON = os.getenv("FB_COOKIES_JSON", "[]")
try:
    FB_COOKIES = json.loads(FB_COOKIES_JSON)
except Exception as e:
    logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å FB_COOKIES_JSON: %s", e)
    FB_COOKIES = []

APIFY_MIN_DELAY = int(os.getenv("APIFY_MIN_DELAY", "1"))
APIFY_MAX_DELAY = int(os.getenv("APIFY_MAX_DELAY", "10"))

POLL_INTERVAL_SECONDS = int(os.getenv("POLL_INTERVAL_SECONDS", "600"))

_seen_hashes: set[str] = set()


# ---------- –£–¢–ò–õ–ò–¢–´ ----------

def today_str() -> str:
    return date.today().isoformat()  # 'YYYY-MM-DD'


def is_today(created_at) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞—Ç–∞ –ø–æ—Å—Ç–∞ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ —Å–µ–≥–æ–¥–Ω—è—à–Ω–µ–º—É –¥–Ω—é.

    –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å:
    - ISO-—Å—Ç—Ä–æ–∫–∏ (—Å –∏–ª–∏ –±–µ–∑ 'Z')
    - timestamp –≤ —Å–µ–∫—É–Ω–¥–∞—Ö –∏–ª–∏ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö.
    –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è ‚Äî —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ –ø–æ—Å—Ç –ù–ï —Å–µ–≥–æ–¥–Ω—è—à–Ω–∏–π.
    """
    if not created_at:
        return False

    s = str(created_at)

    # ISO —Ñ–æ—Ä–º–∞—Ç
    try:
        dt = datetime.fromisoformat(s.replace("Z", "+00:00"))
        return dt.date() == date.today()
    except Exception:
        pass

    # timestamp (—Å–µ–∫—É–Ω–¥—ã –∏–ª–∏ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã)
    try:
        ts = float(s)
        if ts > 1e12:  # –æ—á–µ–Ω—å –∫—Ä—É–ø–Ω–æ–µ —á–∏—Å–ª–æ ‚Äî —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã
            ts /= 1000.0
        dt = datetime.utcfromtimestamp(ts)
        return dt.date() == date.today()
    except Exception:
        return False


def get_fb_groups() -> List[str]:
    """
    –û–∂–∏–¥–∞–µ–º—ã–π –æ—Ç–≤–µ—Ç –º–∏–Ω–∏–∞–ø–ø–∞:
    {
      "groups": [
        { "id": 1, "group_url": "...", "enabled": true },
        ...
      ]
    }
    """
    try:
        logger.info("–ó–∞–ø—Ä–∞—à–∏–≤–∞—é FB-–≥—Ä—É–ø–ø—ã –∏–∑ %s", FB_GROUPS_API_URL)
        resp = requests.get(FB_GROUPS_API_URL, timeout=30)
        resp.raise_for_status()
    except Exception as e:
        logger.error("‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ FB-–≥—Ä—É–ø–ø: %s", e)
        return []

    try:
        data = resp.json()
    except Exception as e:
        logger.error("‚ùå –û—à–∏–±–∫–∞ JSON –ø—Ä–∏ —Ä–∞–∑–±–æ—Ä–µ FB-–≥—Ä—É–ø–ø: %s", e)
        return []

    groups_raw = data.get("groups") or []
    urls: List[str] = []

    for g in groups_raw:
        if not g.get("enabled", True):
            continue
        url = (g.get("group_url") or g.get("group_id") or "").strip()
        if not url:
            continue
        urls.append(url)

    logger.info("–ù–∞–π–¥–µ–Ω–æ %d –≤–∫–ª—é—á—ë–Ω–Ω—ã—Ö FB-–≥—Ä—É–ø–ø", len(urls))
    return urls


def hash_post(text: str, url: str | None) -> str:
    base = (text or "").strip()
    if url:
        base += f"::{url}"
    return str(abs(hash(base)))


# ---------- –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–ï –° MINIAPP ----------

def send_job_to_miniapp(
    text: str,
    post_url: str | None,
    created_at: str | None,
    group_url: str | None,
    author_url: str | None,
) -> None:
    """
    –®–ª—ë–º –ø–æ—Å—Ç –≤ –º–∏–Ω–∏–∞–ø–ø –Ω–∞ /post.

    –ò—Å–ø–æ–ª—å–∑—É–µ–º:
    - text        -> —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏/–ø–æ—Å—Ç–∞
    - url         -> –∫–Ω–æ–ø–∫–∞ "–ü–µ—Ä–µ–π—Ç–∏ –∫ –ø–æ—Å—Ç—É"
    - author_url  -> –∫–ª–∞–¥—ë–º –≤ sender_username, —á—Ç–æ–±—ã –º–∏–Ω–∏–∞–ø–ø –º–æ–≥ —Å–¥–µ–ª–∞—Ç—å –∫–Ω–æ–ø–∫—É "–ù–∞–ø–∏—Å–∞—Ç—å –∞–≤—Ç–æ—Ä—É"
    """
    endpoint = f"{API_BASE_URL}/post"
    headers = {
        "Content-Type": "application/json",
        "X-API-KEY": API_SECRET,
    }

    payload: Dict[str, Any] = {
        "source": "facebook",
        "source_name": group_url or "facebook_group",
        "external_id": post_url or (created_at or ""),
        "url": post_url,
        "text": text,
        # —Å—é–¥–∞ –ø–µ—Ä–µ–¥–∞—ë–º —Å—Å—ã–ª–∫—É –Ω–∞ –ø—Ä–æ—Ñ–∏–ª—å –∞–≤—Ç–æ—Ä–∞
        "sender_username": author_url,
        "created_at": created_at,
    }

    try:
        resp = requests.post(endpoint, json=payload, headers=headers, timeout=30)
        resp.raise_for_status()
        logger.info("‚úÖ –£—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ –ø–æ—Å—Ç –≤ –º–∏–Ω–∏–∞–ø–ø: %s", post_url)
    except Exception as e:
        logger.error("‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –ø–æ—Å—Ç–∞ –≤ –º–∏–Ω–∏–∞–ø–ø: %s", e)


# ---------- –í–´–ó–û–í APIFY –ê–ö–¢–û–†–ê ----------

def call_apify_for_group(group_url: str) -> List[Dict[str, Any]]:
    """
    –í—ã–∑—ã–≤–∞–µ—Ç actor (curious_coder/facebook-post-scraper) —Å input:
      - cookie
      - maxDelay, minDelay
      - proxy.useApifyProxy = true
      - scrapeGroupPosts.groupUrl
      - scrapeUntil = —Å–µ–≥–æ–¥–Ω—è
      - sortType = new_posts
    """
    endpoint = (
        f"https://api.apify.com/v2/acts/{APIFY_ACTOR_ID}/run-sync-get-dataset-items"
        f"?token={APIFY_TOKEN}"
    )

    actor_input = {
        "cookie": FB_COOKIES,
        "maxDelay": APIFY_MAX_DELAY,
        "minDelay": APIFY_MIN_DELAY,
        "proxy": {
            "useApifyProxy": True,
        },
        "scrapeGroupPosts.groupUrl": group_url,
        "scrapeUntil": today_str(),  # –¥–æ —Å–µ–≥–æ–¥–Ω—è—à–Ω–µ–≥–æ –¥–Ω—è
        "sortType": "new_posts",
    }

    logger.info("‚ñ∂Ô∏è –í—ã–∑–æ–≤ Apify –¥–ª—è –≥—Ä—É–ø–ø—ã %s", group_url)
    try:
        resp = requests.post(endpoint, json=actor_input, timeout=600)
        resp.raise_for_status()
    except Exception as e:
        logger.error("‚ùå –û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ Apify –¥–ª—è %s: %s", group_url, e)
        return []

    try:
        data = resp.json()
    except Exception as e:
        logger.error("‚ùå JSON-–æ—à–∏–±–∫–∞ –æ—Ç Apify (%s): %s", group_url, e)
        return []

    # run-sync-get-dataset-items –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å —Å–ø–∏—Å–æ–∫ –∏–ª–∏ –æ–±—ä–µ–∫—Ç —Å items
    if isinstance(data, list):
        items = data
    elif isinstance(data, dict) and "items" in data:
        items = data["items"]
    else:
        logger.warning(
            "–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ Apify –¥–ª—è %s: %r", group_url, data
        )
        return []

    logger.info("–ü–æ–ª—É—á–µ–Ω–æ %d —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –æ—Ç Apify –¥–ª—è %s", len(items), group_url)
    return items


# ---------- –û–°–ù–û–í–ù–û–ô –¶–ò–ö–õ ----------

def process_cycle():
    group_urls = get_fb_groups()
    if not group_urls:
        logger.warning("–ù–µ—Ç FB-–≥—Ä—É–ø–ø –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞, –ø—Ä–æ–ø—É—Å–∫–∞—é —Ü–∏–∫–ª")
        return

    total_sent = 0

    for group_url in group_urls:
        items = call_apify_for_group(group_url)

        for item in items:
            # 1) —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞
            text = (
                item.get("text")
                or item.get("message")
                or item.get("content")
                or item.get("postText")
                or ""
            )

            # 2) —Å—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ—Å—Ç
            post_url = (
                item.get("url")
                or item.get("postUrl")
                or item.get("post_url")
            )

            # 3) –¥–∞—Ç–∞/–≤—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è
            created_at_raw = (
                item.get("createdAt")
                or item.get("timestamp")
                or item.get("created_time")
            )

            # –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ —Å–µ–≥–æ–¥–Ω—è—à–Ω–∏–µ –ø–æ—Å—Ç—ã
            if not is_today(created_at_raw):
                continue

            # 4) —Å—Å—ã–ª–∫–∞ –Ω–∞ –∞–≤—Ç–æ—Ä–∞ (user.url)
            user_data = item.get("user") or {}
            author_url = user_data.get("url")

            # 5) "–≥—Ä—É–ø–ø–∞" ‚Äî –¥–ª—è source_name
            group_field = (
                item.get("groupUrl")
                or item.get("group_url")
                or group_url
            )

            # 6) –∑–∞—â–∏—Ç–∞ –æ—Ç –¥—É–±–ª–µ–π
            h = hash_post(text, post_url)
            if h in _seen_hashes:
                logger.info("üîÅ –î—É–±–ª–∏–∫–∞—Ç –ø–æ—Å—Ç–∞ (hash=%s), –ø—Ä–æ–ø—É—Å–∫–∞—é", h)
                continue
            _seen_hashes.add(h)

            # 7) –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ –º–∏–Ω–∏–∞–ø–ø
            send_job_to_miniapp(
                text=text,
                post_url=post_url,
                created_at=str(created_at_raw) if created_at_raw is not None else None,
                group_url=group_field,
                author_url=author_url,
            )
            total_sent += 1

    logger.info("‚úÖ –¶–∏–∫–ª –∑–∞–≤–µ—Ä—à—ë–Ω, –≤—Å–µ–≥–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –ø–æ—Å—Ç–æ–≤ –≤ –º–∏–Ω–∏–∞–ø–ø: %d", total_sent)


def main():
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ Facebook Job Parser —á–µ—Ä–µ–∑ Apify actor %s", APIFY_ACTOR_ID)
    while True:
        try:
            process_cycle()
        except Exception as e:
            logger.error("‚ùå –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ: %s", e)
        logger.info("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ %d —Å–µ–∫—É–Ω–¥ –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ü–∏–∫–ª–∞", POLL_INTERVAL_SECONDS)
        time.sleep(POLL_INTERVAL_SECONDS)


if __name__ == "__main__":
    main()
