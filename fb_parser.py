# fb_parser.py
import os
import time
import logging
import hashlib
from urllib.parse import urlparse

from facebook_scraper import get_posts
import requests

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - fb_parser - %(levelname)s - %(message)s",
)
log = logging.getLogger(__name__)

# ====================== ENV ======================

# URL –º–∏–Ω–∏–∞–ø–ø–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä:
# https://job-miniapp-service-production.up.railway.app
API_BASE_URL = os.getenv("API_BASE_URL")

# –î–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å API_SECRET –≤ –º–∏–Ω–∏–∞–ø–ø–µ
API_SECRET = os.getenv("API_SECRET", "mvp-secret-key-2024-xyz")

JOB_KEYWORDS = [
    kw.strip().lower()
    for kw in os.getenv(
        "JOB_KEYWORDS",
        "–≤–∞–∫–∞–Ω—Å–∏—è,—Ä–∞–±–æ—Ç–∞,job,hiring,remote,developer,–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç,amazon",
    ).split(",")
    if kw.strip()
]

CHECK_INTERVAL_MINUTES = int(os.getenv("CHECK_INTERVAL_MINUTES", "5"))
MAX_POSTS_PER_GROUP = int(os.getenv("MAX_POSTS_PER_GROUP", "20"))

# –°—ã—Ä—ã–µ cookies —Å—Ç—Ä–æ–∫–æ–π, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –ø–∞—Ä—Å–∏—Ç—å –∑–∞–∫—Ä—ã—Ç—ã–µ/–ø—Ä–∏–≤–∞—Ç–Ω—ã–µ –≥—Ä—É–ø–ø—ã:
# "key1=value1; key2=value2; ..."
FACEBOOK_COOKIES = os.getenv("FACEBOOK_COOKIES")


# ====================== –í—Å–ø–æ–º–æ–≥–∞–ª–∫–∏ ======================

def get_cookies_dict():
    """
    –ü—Ä–æ—Å—Ç–æ–π —Ä–∞–∑–±–æ—Ä cookies —Ñ–æ—Ä–º–∞—Ç–∞ "key1=value1; key2=value2"
    """
    if not FACEBOOK_COOKIES:
        return None
    cookies: dict[str, str] = {}
    for part in FACEBOOK_COOKIES.split(";"):
        part = part.strip()
        if not part or "=" not in part:
            continue
        k, v = part.split("=", 1)
        cookies[k.strip()] = v.strip()
    return cookies


def get_fb_groups():
    """
    –¢—è–Ω–µ–º —Å–ø–∏—Å–æ–∫ FB-–≥—Ä—É–ø–ø –∏–∑ miniapp-—Å–µ—Ä–≤–∏—Å–∞.
    GET {API_BASE_URL}/api/groups
    """
    if not API_BASE_URL:
        raise RuntimeError("API_BASE_URL is not set")

    url = f"{API_BASE_URL.rstrip('/')}/api/groups"
    resp = requests.get(url, timeout=30)
    resp.raise_for_status()
    data = resp.json()

    groups: list[tuple[str, str]] = []
    for g in data.get("groups", []):
        if g.get("enabled"):
            groups.append((g["group_id"], g["group_name"]))
    return groups


def text_matches_keywords(text: str) -> bool:
    t = (text or "").lower()
    return any(kw in t for kw in JOB_KEYWORDS)


def build_external_id(group_id: str, post_id: str) -> str:
    # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    raw = f"fb:{group_id}:{post_id}"
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()


def send_job(group_name: str, group_link: str, post: dict) -> None:
    """
    –û—Ç–ø—Ä–∞–≤–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ miniapp /post
    """
    if not API_BASE_URL:
        raise RuntimeError("API_BASE_URL is not set")

    post_id = post.get("post_id") or post.get("post_url") or ""
    text = post.get("text") or ""
    post_url = post.get("post_url") or group_link

    external_id = build_external_id(group_link, str(post_id))

    created_at = None
    if post.get("time"):
        try:
            created_at = post["time"].isoformat()
        except Exception:
            created_at = None

    payload = {
        "source": "facebook",
        "source_name": group_name,
        "external_id": external_id,
        "url": post_url,
        "text": text,
        "created_at": created_at,
    }

    headers = {
        "Content-Type": "application/json",
        "X-API-KEY": API_SECRET,
    }

    resp = requests.post(
        f"{API_BASE_URL.rstrip('/')}/post",
        json=payload,
        headers=headers,
        timeout=30,
    )
    if resp.status_code == 200:
        data = resp.json()
        if data.get("status") == "duplicate":
            log.info("üîÅ –£–∂–µ –µ—Å—Ç—å —Ç–∞–∫–æ–π –ø–æ—Å—Ç: %s", external_id)
        else:
            log.info("‚úÖ –ù–æ–≤–∞—è –≤–∞–∫–∞–Ω—Å–∏—è –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞: %s", external_id)
    else:
        log.error("‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤–∞–∫–∞–Ω—Å–∏–∏: %s %s", resp.status_code, resp.text)


def parse_group(group_link: str, group_name: str, cookies: dict | None):
    """
    –ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø—ã.
    group_link ‚Äî —Ç–æ, —á—Ç–æ —Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ fb_groups.group_id (–º–æ–∂–µ—Ç –±—ã—Ç—å –∏ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è —Å—Å—ã–ª–∫–∞).
    """
    log.info("üîç –ü–∞—Ä—Å–∏–º –≥—Ä—É–ø–ø—É: %s (%s)", group_name, group_link)

    parsed = urlparse(group_link)
    group = parsed.path.strip("/") or group_link

    count = 0
    for post in get_posts(
        group=group,
        pages=1,
        cookies=cookies,
        options={"allow_extra_requests": False},
    ):
        text = post.get("text") or ""
        if not text_matches_keywords(text):
            continue

        send_job(group_name, group_link, post)
        count += 1

        if count >= MAX_POSTS_PER_GROUP:
            break

    log.info("üìå –î–ª—è %s –Ω–∞–π–¥–µ–Ω–æ %s –ø–æ—Å—Ç–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º", group_name, count)
    return count


def run_loop():
    cookies = get_cookies_dict()
    log.info("üöÄ –ó–∞–ø—É—Å–∫ Facebook Job Parser")
    log.info("–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: %s", JOB_KEYWORDS)

    while True:
        try:
            groups = get_fb_groups()
            log.info("–ù–∞–π–¥–µ–Ω–æ %s –∞–∫—Ç–∏–≤–Ω—ã—Ö –≥—Ä—É–ø–ø", len(groups))

            total_posts = 0
            for group_link, group_name in groups:
                total_posts += parse_group(group_link, group_name, cookies)
                time.sleep(2)

            log.info("‚úÖ –¶–∏–∫–ª –∑–∞–≤–µ—Ä—à—ë–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ %s –ø–æ—Å—Ç–æ–≤", total_posts)
        except Exception as e:
            log.exception("‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: %s", e)

        log.info("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ %s –º–∏–Ω—É—Ç...", CHECK_INTERVAL_MINUTES)
        time.sleep(CHECK_INTERVAL_MINUTES * 60)


if __name__ == "__main__":
    run_loop()
